{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Metodologies for Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "- Lorenzo Dell'Oro\n",
    "- Giovanni Toto\n",
    "- Gian Luca Vriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>KEY IDEA AND OBJECTIVE OF THE PROJECT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>LIBRARIES</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Guardian dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION TO DATA</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an API key in order to download *Guardian* articles using the [official API](https://open-platform.theguardian.com/): in the following block, you should replace `'test'` with your API key; it can be easily obtained [here](https://open-platform.theguardian.com/access/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.i. Download using API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "query = {'type': 'article',\n",
    "         'q': '\"climate change\"',\n",
    "         'section': 'environment',\n",
    "         'from-date': \"2013-01-01\",\n",
    "         'to-date': \"2022-12-31\",\n",
    "         'lang': 'en',\n",
    "         'order-by': 'oldest',\n",
    "         'page-size': 200,\n",
    "         'show-fields': 'all',\n",
    "         'api-key': api_key}\n",
    "\n",
    "from src.file_io import download_guardian\n",
    "corpus = download_guardian(query, 'data/raw/guardian_environment.csv')\n",
    "\"\"\"\n",
    "corpus = pd.read_csv('data/raw/guardian_environment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.ii Selection of the variable of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`corpus` is a `pandas` dataset containing several pieces of information for each document, most of which are not relevant to our analysis; hence, we decided to keep only the following fields:\n",
    "- document-related:\n",
    "    - `id`: identifier\n",
    "    - `year`: year of publication on the website (`webPublicationDate`)\n",
    "    - `where`: original place on which the article was published (`fields_publication`)\n",
    "    - `author`: author (`fields_byline`)\n",
    "- text-related:\n",
    "    - `headline`: title (`fields_headline`)\n",
    "    - `standfirst`: summary (`fields_standfirst`)\n",
    "    - `body`: text with tags (`fields_body`)\n",
    "    - `bodyText`: text without tags (`fields_bodyText`)\n",
    "- count-related:\n",
    "    - `wordcount`: number of words in the body (`fields_wordcount`)\n",
    "    - `charcount`: number of characters in the body without tags (`fields_charcount`)\n",
    "    \n",
    "We reported in parentheses the original name of the field or, in the case of `year`, the name of the field from which `year` was obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_of_interest = ['id', 'webPublicationDate', 'fields_publication', 'fields_byline',\n",
    "                   'fields_headline', 'fields_standfirst', 'fields_body', 'fields_bodyText',\n",
    "                   'fields_wordcount', 'fields_charCount']\n",
    "# select columns of interest\n",
    "corpus = corpus.loc[:, col_of_interest]\n",
    "# convert 'webPublicationDate' field to year\n",
    "corpus['webPublicationDate'] = pd.to_datetime(corpus['webPublicationDate']).dt.year\n",
    "# rename columns\n",
    "new_colnames = {'webPublicationDate': 'year', 'fields_publication': 'where', 'fields_byline': 'author',\n",
    "                'fields_headline': 'headline', 'fields_standfirst': 'standfirst', 'fields_body': 'body', 'fields_bodyText':'bodyText',\n",
    "                'fields_wordcount': 'wordcount', 'fields_charCount': 'charcount'}\n",
    "corpus.rename(columns=new_colnames, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we remove the articles which do not have a body, i.e. the value of `bodyText` is `nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus[corpus['bodyText'].isna() == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if an article does not have the headline and/or standfirst, we replace the `nan`s with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['headline'][corpus['headline'].isna() == True] = ''\n",
    "corpus['standfirst'][corpus['standfirst'].isna() == True] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.ii. Pre-processing of the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>PRE-PROCESSING OF TEXTS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we consider the texts of the article downloaded in the previous section and we process them in order to make them compatible with *ETM* and *DETM*. In particular, we want to obtain two list of strings:\n",
    "- `timestamps` containing the timestamps of the articles, i.e. the years in which they were published;\n",
    "- `docs` containing the processed texts of the articles, i.e. the headline, standfirst and body.\n",
    "\n",
    "\n",
    "First, we obtain `timestamps` variable from year field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = [str(y) for y in corpus['year'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we process the texts in order to obtain an input compatible with text analysis approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# regex for urls; source: https://stackoverflow.com/a/50790119\n",
    "url_regex = r\"\\b((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\\b\"\n",
    "\n",
    "docs = corpus['headline'] + \" \" + corpus['standfirst'] + \" \" + corpus['bodyText']\n",
    "docs = docs.tolist()\n",
    "docs = [re.sub(url_regex, '', docs[doc].lower()) for doc in range(len(docs))]\n",
    "docs = [[w.translate(str.maketrans('', '', string.punctuation + \"0123456789\")) for w in docs[doc].split()] for doc in range(len(docs))]\n",
    "docs = [[w for w in docs[doc] if len(w)>1] for doc in range(len(docs))]\n",
    "docs = [\" \".join(docs[doc]) for doc in range(len(docs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use `preprocessing` function contained in `src/preprocessing.py` module, which creates files compatible with *ETM* and *DETM*. Before launching the function, we need to import stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "Preparing data:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\"path_save\" not valid: the folder already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ceab6750e7f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m preprocessing(data_path=\"data/guardian_environment\", docs=docs, timestamps=timestamps, stopwords=stopwords,\n\u001b[1;32m----> 7\u001b[1;33m               min_df=10, max_df=0.7, data_split=[0.7, 0.2, 0.1], seed=28)\n\u001b[0m",
      "\u001b[1;32mD:\\pmda\\src\\preprocessing.py\u001b[0m in \u001b[0;36mpreprocessing\u001b[1;34m(data_path, docs, timestamps, stopwords, min_df, max_df, data_split, seed)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_save\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"path_save\" not valid: the folder already exists.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Check docs and timestamps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: \"path_save\" not valid: the folder already exists."
     ]
    }
   ],
   "source": [
    "# Read stopwords\n",
    "with open(\"./data/stops.txt\", \"r\") as f:\n",
    "    stopwords = f.read().split('\\n')\n",
    "# Pre-processing\n",
    "from src.preprocessing import preprocessing\n",
    "preprocessing(data_path=\"data/guardian_environment\", docs=docs, timestamps=timestamps, stopwords=stopwords,\n",
    "              min_df=10, max_df=0.7, data_split=[0.7, 0.2, 0.1], seed=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function also divides the corpus into train, test and validation set: we are going to consider the training set only for the exploratory analyses in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "EXPLORATORY ANALYSIS OF THE <b>TRAIN</b> CORPUS:\n",
    "\n",
    "- tabella con info corpus (num documenti, num timestamps, documenti per timestamp, ...)\n",
    "- distribuzione lunghezza documenti (numero parole)\n",
    "- parole più frequenti (word cloud)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, we remove all variables that are no longer used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del api_key, col_of_interest, docs, new_colnames, timestamps, url_regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said before, here we consider train set only: we do not need test and validation set from now on, so we replace the whole corpus, stored in `corpus` variable, with the train set. We also store the vocabulary of the train set in `vocab` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/guardian_environment/info.json', 'r') as f:\n",
    "    info = json.load(f)\n",
    "    corpus = corpus.iloc[sorted(info['indices_tr']), :]\n",
    "    docs_bow = info['docs_tr']\n",
    "    vocab = info['vocab_tr']\n",
    "    del info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPLORATORY ANALYSIS ON `corpus` variable** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Code wordcloud](https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#from-strings-to-vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>BRIEF DESCRIPTION OF THE DIFFERENT APPROACHES</font><br>\n",
    "<font color='blue'>EMBEDDING FITTING</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static word embeddings:\n",
    "- *GloVe* <br> (Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP.)\n",
    "- *Word2vec* (*CBOW* & *Skip Gram*) <br> (Mikolov, Tomas & Chen, Kai & Corrado, G.s & Dean, Jeffrey. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of Workshop at ICLR. 2013.) <br> (Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 (NIPS'13). Curran Associates Inc., Red Hook, NY, USA, 3111–3119.)\n",
    "- *Sent2Vec* <br> (Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. 2018. Unsupervised learning of sentence embeddings using compositional n-gram features. In NAACL-HLT.)\n",
    "- *fastText* <br>\n",
    "\n",
    "Static word embeddings obtained from dynamic ones:\n",
    "- Prakhar Gupta and Martin Jaggi. 2021. Obtaining Better Static Word Embeddings Using Contextual Embedding Models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5241–5253, Online. Association for Computational Linguistics. ([GitHub](https://github.com/epfml/X2Static))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the use the same embedding space for both *ETM* and *DETM*, so we first fit the word embeddings and then we provide them as input to the topic models. We import here the required functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText, KeyedVectors, Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from src.file_io import save_embeddings\n",
    "\n",
    "embedding_path, embedding_file = 'embeddings/', 'guardian_environment'\n",
    "embedding_path = embedding_path + embedding_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.i. Word2vec on train corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit CBOW and skip gram on train corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sg=0: CBOW\n",
    "cbow = Word2Vec(sentences=docs_bow, min_count=100, sg=0, size=100, iter=5, workers=5, negative=10, window=4)\n",
    "save_embeddings(emb_model=cbow, emb_file=embedding_path+'_cbow.txt', vocab=vocab)\n",
    "# sg=1: skip-gram\n",
    "skipgram = Word2Vec(sentences=docs_bow, min_count=100, sg=1, size=100, iter=5, workers=5, negative=10, window=4)\n",
    "save_embeddings(emb_model=skipgram, emb_file=embedding_path+'_skipgram.txt', vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.ii. FastText on train corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastText(min_count=100, vector_size=300, iter=5, negative=10, window=4)\n",
    "fasttext.build_vocab(corpus_file=docs_bow)\n",
    "fasttext.train(corpus_file=docs_bow, total_examples=fasttext.corpus_count, total_words=fasttext.corpus_total_words)\n",
    "save_embeddings(emb_model=fasttext, emb_file=embedding_path+'_fasttext.txt', vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.iii. Google’s Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the pre-trained word embedding from [here](https://code.google.com/archive/p/word2vec/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_word2vec = KeyedVectors.load_word2vec_format('embeddings/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "save_embeddings(emb_model=google_word2vec, emb_file=embedding_path+'_google_word2vec.txt', vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.iv. Stanford’s GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the pre-trained word embedding from [here](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word2vec file\n",
    "glove_input_file = 'embeddings/glove.6B.300d.txt'\n",
    "word2vec_output_file = 'embeddings/glove.6B.300d.bin'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "# load embeddings\n",
    "glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "save_embeddings(emb_model=glove, emb_file=embedding_path+'_glove.txt', vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.v. Obtaining Better Static Word Embeddings Using Contextual Embedding Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Topic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION TO MODEL ESTIMATION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_list = [10, 20, 30, 40, 50]\n",
    "embedding_list = ['cbow', 'skipgram', 'fasttext', 'google_word2vec', 'glove']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.i. Embedded Topic Model (ETM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>BRIEF DESCRIPTION OF THE MODEL</font><br>\n",
    "<font color='blue'>MODEL ESTIMATION</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block allows to train *ETM*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.main_ETM import main_ETM\n",
    "main_ETM(dataset='un-general-debates', data_path='data/un-general-debates', save_path='results',\n",
    "         emb_file='data/un-general-debates_embeddings.txt', model_file='ETM_K50_un-general-debates', batch_size=1000,\n",
    "         mode='train', num_topics=50, train_embeddings=0, epochs=100, visualize_every=1000, tc=False, td=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block allows to evaluate *ETM*, i.e.,\n",
    "- compute *topic coherence* on the top 10 words of each topic;\n",
    "- compute *topic diversity* on the top 25 words of each topic,\n",
    "- compute the ranking of the most used topics in the train corpus;\n",
    "- compute the top `num_words` words per topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.ii. Dynamic Embedded Topic Model (DETM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>BRIEF DESCRIPTION OF THE MODEL</font><br>\n",
    "<font color='blue'>MODEL ESTIMATION</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory problems:** DETM rquires too much memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.main_DETM import main_DETM\n",
    "main_DETM(dataset='un-general-debates', data_path='data/un-general-debates', save_path='data',\n",
    "          emb_path='data/un-general-debates_embeddings.txt', mode='train', batch_size=1000,\n",
    "          num_topics=50, train_embeddings=0, epochs=50, visualize_every=1000, tc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION ON HOW WE WANT TO COMPARE MODELS</font>\n",
    "\n",
    "**The idea is introduced very well in \"Topic modeling in embedding spaces\": let's copy from there!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block allows to evaluate the model trained in the previous section; in particular, it saves a file, called `<model_name>_parameters.pt` containing:\n",
    "- `tc` contains the topic coherence\n",
    "- `td` contains the topic diversity\n",
    "- `rho` contains the word embeddings (row=embedding)\n",
    "- `model.alphas.weight` contains the topic embeddings (row=embedding)\n",
    "- `beta` contains the topic-word distributions (row=distribution)\n",
    "- `theta` contains the document-topic distribution (row=distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.main_ETM import main_ETM\n",
    "main_ETM(dataset='un-general-debates', data_path='data/un-general-debates', save_path='results',\n",
    "         emb_file='data/un-general-debates_embeddings.txt', model_file='ETM_K50_un-general-debates', mode='eval',\n",
    "         load_from='results/ETM_K50_un-general-debates',\n",
    "         num_topics=50, train_embeddings=0, epochs=100, visualize_every=1000, num_words=10, tc=True, td=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import load as torch_load\n",
    "loaded = torch_load(\"results/ETM_K50_un-general-debates_parameters.pt\")\n",
    "print(loaded.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TC  :\", loaded['tc'])\n",
    "print(\"TD  :\", loaded['td'])\n",
    "print(\"VxE :\", loaded['rho'].shape)\n",
    "print(\"TxE :\", loaded['alpha'].shape)\n",
    "print(\"TxV :\", loaded['beta'].shape)\n",
    "print(\"DxT :\", loaded['theta'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 2, 3], [2, 3, 1]])\n",
    "display(x)\n",
    "#print(np.argsort(x, axis=1))\n",
    "print(np.argsort(-1 * x, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.i. Quantitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>COMPUTATION OF VARIOUS METRICS AND CONSTRUCTION OF GRAPHS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.ii. Qualitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>INTERPRETATION OF TOPICS AND DOCUMENT REPRESENTATION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>FINAL REMARKS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2019). The dynamic embedded topic model. arXiv preprint arXiv:1907.05545. [Arxiv link](https://arxiv.org/abs/1907.05545)\n",
    "\n",
    "Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2020). Topic modeling in embedding spaces. Transactions of the Association for Computational Linguistics, 8, 439-453. [ACM Anthology](https://aclanthology.org/2020.tacl-1.29/),  [Arxiv link](https://arxiv.org/abs/1907.04907)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmda",
   "language": "python",
   "name": "pmda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
