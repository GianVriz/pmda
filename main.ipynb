{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Metodologies for Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "- Lorenzo Dell'Oro\n",
    "- Giovanni Toto\n",
    "- Gian Luca Vriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>KEY IDEA AND OBJECTIVE OF THE PROJECT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>LIBRARIES</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                                   # 2\n",
    "import string                                # 2\n",
    "from src.preprocessing import preprocessing  # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION TO DATA</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>PRE-PROCESSING OF TEXTS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import the dataset from a file (txt/csv/...); this is an example with [UN General Debates corpus](https://www.kaggle.com/datasets/unitednations/un-general-debates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DOCS = 3000\n",
    "min_df = 10\n",
    "\n",
    "# Data type\n",
    "flag_split_by_paragraph = False  # whether to split documents by paragraph\n",
    "    \n",
    "# Read raw data (https://www.kaggle.com/datasets/unitednations/un-general-debates)\n",
    "print('reading raw data...')\n",
    "with open('./data/raw/un-general-debates.csv', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',', quotechar='\"')\n",
    "    line_count = 0\n",
    "    all_timestamps_ini = []\n",
    "    all_docs_ini = []\n",
    "    for row in csv_reader:\n",
    "        # skip header\n",
    "        if(line_count>0):\n",
    "            all_timestamps_ini.append(row[1])\n",
    "            all_docs_ini.append(row[3].encode(\"ascii\", \"ignore\").decode())\n",
    "        line_count += 1\n",
    "        if line_count==N_DOCS-1:  ###########\n",
    "            break                 ###########\n",
    "\n",
    "if flag_split_by_paragraph:\n",
    "    print('splitting by paragraphs...')\n",
    "    docs = []\n",
    "    timestamps = []\n",
    "    for dd, doc in enumerate(all_docs_ini):\n",
    "        splitted_doc = doc.split('.\\n')\n",
    "        for ii in splitted_doc:\n",
    "            docs.append(ii)\n",
    "            timestamps.append(all_timestamps_ini[dd])\n",
    "else:\n",
    "    docs = all_docs_ini\n",
    "    timestamps = all_timestamps_ini\n",
    "\n",
    "del all_docs_ini\n",
    "del all_timestamps_ini\n",
    "\n",
    "print('  number of documents: {}'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to pre-processing with gensim: [link](https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#from-strings-to-vectors)\n",
    "\n",
    "The real pre-processing starts here: in short, we want to get strings without strange characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "print('removing punctuation...')\n",
    "docs = [[w.lower().replace(\"â€™\", \" \").replace(\"'\", \" \").translate(str.maketrans('', '', string.punctuation + \"0123456789\")) for w in docs[doc].split()] for doc in range(len(docs))]\n",
    "docs = [[w for w in docs[doc] if len(w)>1] for doc in range(len(docs))]\n",
    "docs = [\" \".join(docs[doc]) for doc in range(len(docs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use `preprocessing` function contained in `src/preprocessing.py` module, which creates files compatible with *ETM* and *DETM*. We will use these files also to perform explorative analysis. Before launching the function, we need to import stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stopwords\n",
    "with open(\"./data/stops.txt\", \"r\") as f:\n",
    "    stopwords = f.read().split('\\n')\n",
    "# Pre-processing\n",
    "preprocessing(data_path=\"data/un-general-debates\", docs=docs, timestamps=timestamps, stopwords=stopwords,\n",
    "              min_df=min_df, max_df=0.7, data_split=[0.85, 0.1, 0.05], seed=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark1:** `docs` and `timestamps` are two lists of strings containing the same number of elements; in particular, `docs` contains the documents of the corpus, `timestamps` their timestamps.\n",
    "\n",
    "**remark2:** The function also divides the corpus into train, test and validation set: below we will consider the training set for exploratory analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory analysis of the processed corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>EXPLORATORY ANALYSIS OF THE **TRAIN** CORPUS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import vocabulary of the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.file_io import load_vocab\n",
    "word2id, id2word = load_vocab(\"data/un-general-debates/vocab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider the train corpus only, i.e. we are interested in the following files generated by `preprocessing` function:\n",
    "- `bow_tr_tokens`: index of the different words observed in the documents of the train set;\n",
    "- `bow_tr_counts`: occurrences of the different words observed in the documents of the train set;\n",
    "- `bow_tr_timestamps`: timestamps of the documents of the train set;\n",
    "- `timestamps.txt`: different observed timestamps;\n",
    "- `vocab.txt`: vocabulary of the train set.\n",
    "\n",
    "**remark:** the first 3 files exist also for test and validation set, however they are not relevant to exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "path = os.path.join('data', 'un-general-debates', 'min_df_'+str(min_df))\n",
    "bow_tr_tokens = loadmat(os.path.join(path, 'bow_tr_tokens'))['tokens'].squeeze()\n",
    "bow_tr_counts = loadmat(os.path.join(path, 'bow_tr_counts'))['counts'].squeeze()\n",
    "bow_tr_timestamps = loadmat(os.path.join(path, 'bow_tr_timestamps'))['timestamps'].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(bow_tr_timestamps))\n",
    "print(len(bow_tr_counts))\n",
    "print(len(bow_tr_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"print(bow_tr_timestamps)\n",
    "for i in range(8):\n",
    "    print(i, \"\\t\", bow_tr_tokens[i].shape, \"\\t\", bow_tr_counts[i].shape)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embeddings and topic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION TO MODEL ESTIMATION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.i. Fitting embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>BRIEF DESCRIPTION OF THE DIFFERENT APPROACHES</font><br>\n",
    "<font color='blue'>EMBEDDING FITTING</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the use the same embedding space for both *ETM* and *DETM*, so we first fit the word embeddings and then we provide them as input. In particular, we fit a simple *skipgram*; this implementation is an adaptation of this [code](https://github.com/adjidieng/ETM/blob/master/skipgram.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bow = [docs[doc].split() for doc in range(len(docs))]  # list of list of strings (BoW representation)\n",
    "\n",
    "# fit embeddings\n",
    "from gensim.models import Word2Vec\n",
    "skipgram = Word2Vec(sentences=docs_bow, min_count=100, sg=1, size=100, iter=5, workers=5, negative=10, window=4)\n",
    "\n",
    "from src.file_io import save_embeddings\n",
    "save_embeddings(emb_model=skipgram, emb_file='data/un-general-debates_embeddings.txt', vocab=list(word2id.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.ii. Embedded Topic Model (ETM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>BRIEF DESCRIPTION OF THE MODEL</font><br>\n",
    "<font color='blue'>MODEL ESTIMATION</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that:\n",
    "- `rho` contains the word embeddings (row=embedding)\n",
    "- `model.alphas.weight` contains the topic embeddings (row=embedding)\n",
    "- `beta` contains the topic-word distributions (row=distribution)\n",
    "- `theta` contains the document-topic distribution (row=distribution)\n",
    "\n",
    "**TO DO:** modify `main_ETM` function to make it return these quantities (when `mode='eval'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block allows to train *ETM*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main_ETM import main_ETM\n",
    "main_ETM(dataset='un-general-debates', data_path='data/un-general-debates', save_path='data',\n",
    "         emb_path='data/un-general-debates_embeddings.txt', mode='train',\n",
    "         num_topics=50, train_embeddings=0, epochs=100, visualize_every=1000, tc=False, td=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block allows to evaluate *ETM*, i.e.,\n",
    "- compute *topic coherence* on the top 10 words of each topic;\n",
    "- compute *topic diversity* on the top 25 words of each topic,\n",
    "- compute the ranking of the most used topics in the train corpus;\n",
    "- compute the top `num_words` words per topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "Training an Embedded Topic Model on UN-GENERAL-DEBATES\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "ckpt: data/etm_un-general-debates_K_50_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0\n",
      "13265  The Vocabulary size is here \n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.0, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (alphas): Linear(in_features=300, out_features=50, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=13265, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=50, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=50, bias=True)\n",
      ")\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 3838.7\n",
      "****************************************************************************************************\n",
      "Computing topic coherence...\n",
      "D:  2548\n",
      "k: 0/50\n",
      "k: 1/50\n",
      "k: 2/50\n",
      "k: 3/50\n",
      "k: 4/50\n",
      "k: 5/50\n",
      "k: 6/50\n",
      "k: 7/50\n",
      "k: 8/50\n",
      "k: 9/50\n",
      "k: 10/50\n",
      "k: 11/50\n",
      "k: 12/50\n",
      "k: 13/50\n",
      "k: 14/50\n",
      "k: 15/50\n",
      "k: 16/50\n",
      "k: 17/50\n",
      "k: 18/50\n",
      "k: 19/50\n",
      "k: 20/50\n",
      "k: 21/50\n",
      "k: 22/50\n",
      "k: 23/50\n",
      "k: 24/50\n",
      "k: 25/50\n",
      "k: 26/50\n",
      "k: 27/50\n",
      "k: 28/50\n",
      "k: 29/50\n",
      "k: 30/50\n",
      "k: 31/50\n",
      "k: 32/50\n",
      "k: 33/50\n",
      "k: 34/50\n",
      "k: 35/50\n",
      "k: 36/50\n",
      "k: 37/50\n",
      "k: 38/50\n",
      "k: 39/50\n",
      "k: 40/50\n",
      "k: 41/50\n",
      "k: 42/50\n",
      "k: 43/50\n",
      "k: 44/50\n",
      "k: 45/50\n",
      "k: 46/50\n",
      "k: 47/50\n",
      "k: 48/50\n",
      "k: 49/50\n",
      "counter:  55\n",
      "num topics:  50\n",
      "Topic Coherence is: 0.04856281947087004\n",
      "Computing topic diversity...\n",
      "Topic diversity is: 0.9072\n",
      "Most used topics:\n",
      "Topic  45 :   0.028124722\n",
      "Topic  32 :   0.026452815\n",
      "Topic   7 :   0.025700025\n",
      "Topic  38 :   0.025480108\n",
      "Topic  44 :   0.024522765\n",
      "Topic   6 :   0.024271864\n",
      "Topic  28 :   0.024093095\n",
      "Topic  21 :   0.023708161\n",
      "Topic  18 :   0.022947103\n",
      "Topic  15 :   0.022460636\n",
      "Topic  47 :   0.02224763\n",
      "Topic   8 :   0.02222349\n",
      "Topic   1 :   0.022098007\n",
      "Topic   0 :   0.021939758\n",
      "Topic  14 :   0.021624159\n",
      "Topic   2 :   0.021337321\n",
      "Topic  16 :   0.021210022\n",
      "Topic  23 :   0.02107834\n",
      "Topic  40 :   0.020860663\n",
      "Topic  37 :   0.020158377\n",
      "Topic  35 :   0.019925157\n",
      "Topic  36 :   0.01991353\n",
      "Topic  29 :   0.01988372\n",
      "Topic  48 :   0.019765962\n",
      "Topic  19 :   0.019712636\n",
      "Topic   4 :   0.019702958\n",
      "Topic   9 :   0.019640699\n",
      "Topic  33 :   0.019549472\n",
      "Topic  30 :   0.019478519\n",
      "Topic   5 :   0.019292539\n",
      "Topic  20 :   0.01886332\n",
      "Topic  31 :   0.018760433\n",
      "Topic  12 :   0.018662507\n",
      "Topic  27 :   0.018398393\n",
      "Topic  11 :   0.018020475\n",
      "Topic  13 :   0.017811678\n",
      "Topic   3 :   0.017733542\n",
      "Topic  46 :   0.017654553\n",
      "Topic  25 :   0.017553033\n",
      "Topic  42 :   0.01754713\n",
      "Topic  24 :   0.017171117\n",
      "Topic  43 :   0.016895587\n",
      "Topic  10 :   0.01658953\n",
      "Topic  22 :   0.016571227\n",
      "Topic  17 :   0.016272498\n",
      "Topic  34 :   0.016017519\n",
      "Topic  26 :   0.01577404\n",
      "Topic  49 :   0.015766075\n",
      "Topic  39 :   0.015000274\n",
      "Topic  41 :   0.01353281\n",
      "\n",
      "Top  10  words per topic:\n",
      "Topic 0: ['basis', 'strengthen', 'increase', 'participation', 'field', 'agreements', 'actions', 'decisions', 'building']\n",
      "Topic 1: ['justice', 'goals', 'sovereignty', 'good', 'lebanon', 'established', 'strong', 'report', 'commitments']\n",
      "Topic 2: ['conditions', 'destruction', 'essential', 'effort', 'latin', 'convinced', 'group', 'hand', 'recognized']\n",
      "Topic 3: ['threat', 'establishment', 'achieved', 'collective', 'play', 'aid', 'kampuchea', 'resolution', 'ocean']\n",
      "Topic 4: ['israel', 'declaration', 'initiative', 'share', 'return', 'israeli', 'colonial', 'decades', 'connection']\n",
      "Topic 5: ['major', 'level', 'race', 'task', 'humanitarian', 'working', 'korea', 'brought', 'sincere']\n",
      "Topic 6: ['europe', 'life', 'afghanistan', 'aimed', 'approach', 'anniversary', 'internal', 'territories', 'number']\n",
      "Topic 7: ['trade', 'held', 'issue', 'results', 'iraq', 'accordance', 'western', 'external', 'operations']\n",
      "Topic 8: ['disarmament', 'developed', 'concerned', 'cent', 'america', 'southern', 'sea', 'occupied', 'confrontation']\n",
      "Topic 9: ['africa', 'debt', 'long', 'talks', 'past', 'join', 'positive', 'sense', 'mechanisms']\n",
      "Topic 10: ['negotiations', 'struggle', 'society', 'contribute', 'interest', 'urgent', 'conflicts', 'making', 'responsibility']\n",
      "Topic 11: ['crisis', 'stability', 'strengthening', 'provide', 'territorial', 'leaders', 'establish', 'millennium', 'main']\n",
      "Topic 12: ['climate', 'growth', 'economy', 'food', 'case', 'caribbean', 'children', 'implementation', 'access']\n",
      "Topic 13: ['african', 'concern', 'tribute', 'created', 'continued', 'serve', 'welcomes', 'sanctions', 'primary']\n",
      "Topic 14: ['settlement', 'policy', 'aggression', 'occupation', 'september', 'deep', 'concrete', 'ensuring', 'subject']\n",
      "Topic 15: ['european', 'sustainable', 'context', 'fundamental', 'universal', 'manner', 'forum', 'number', 'pleased']\n",
      "Topic 16: ['opportunity', 'american', 'step', 'century', 'reason', 'disputes', 'responsibilities', 'increasingly', 'nature']\n",
      "Topic 17: ['terrorism', 'assistance', 'continues', 'majority', 'groups', 'fair', 'required', 'begin', 'restore']\n",
      "Topic 18: ['call', 'poverty', 'high', 'legitimate', 'territory', 'permanent', 'exercise', 'involved', 'remain']\n",
      "Topic 19: ['foreign', 'arms', 'agenda', 'address', 'selfdetermination', 'pay', 'express', 'effects', 'technology']\n",
      "Topic 20: ['financial', 'agreement', 'promote', 'continent', 'position', 'based', 'strategy', 'ways', 'recognize']\n",
      "Topic 21: ['view', 'find', 'governments', 'set', 'women', 'understanding', 'complete', 'events', 'organizations']\n",
      "Topic 22: ['namibia', 'fact', 'apartheid', 'mutual', 'liberation', 'geneva', 'group', 'reduce', 'real']\n",
      "Topic 23: ['special', 'matter', 'tension', 'day', 'proposals', 'desire', 'islands', 'provided', 'asia']\n",
      "Topic 24: ['meeting', 'responsibility', 'entire', 'values', 'constructive', 'challenges', 'mission', 'supports', 'legal']\n",
      "Topic 25: ['means', 'terms', 'lives', 'congratulations', 'undertaken', 'unity', 'differences', 'detente', 'management']\n",
      "Topic 26: ['pacific', 'agreed', 'fully', 'continuing', 'kingdom', 'actively', 'nonproliferation', 'adoption', 'shared']\n",
      "Topic 27: ['military', 'middle', 'ago', 'powers', 'question', 'movement', 'determination', 'provisions', 'concern']\n",
      "Topic 28: ['clear', 'adopted', 'affairs', 'taking', 'committee', 'contribution', 'capacity', 'decision', 'withdrawal']\n",
      "Topic 29: ['delegation', 'forces', 'interests', 'acts', 'achieve', 'secure', 'zone', 'complex', 'de']\n",
      "Topic 30: ['dialogue', 'palestinian', 'solidarity', 'lead', 'principle', 'policies', 'confidence', 'build', 'active']\n",
      "Topic 31: ['relations', 'framework', 'present', 'opportunity', 'face', 'proposal', 'attention', 'man', 'representative']\n",
      "Topic 32: ['independence', 'charter', 'solution', 'parties', 'free', 'nation', 'objectives', 'developments', 'conclusion']\n",
      "Topic 33: ['issues', 'environment', 'central', 'special', 'result', 'peacekeeping', 'activities', 'successful', 'goal']\n",
      "Topic 34: ['south', 'resolutions', 'relations', 'cyprus', 'korean', 'fundamental', 'accepted', 'historic', 'welcomed']\n",
      "Topic 35: ['nuclear', 'weapons', 'common', 'force', 'control', 'long', 'face', 'wars', 'rise']\n",
      "Topic 36: ['democratic', 'freedom', 'power', 'mankind', 'put', 'means', 'independent', 'nonaligned', 'enable']\n",
      "Topic 37: ['law', 'integrity', 'present', 'north', 'live', 'total', 'concerns', 'authority', 'status']\n",
      "Topic 38: ['reform', 'regard', 'past', 'including', 'recently', 'convention', 'refugees', 'spirit', 'programmes']\n",
      "Topic 39: ['democracy', 'challenges', 'based', 'membership', 'potential', 'positive', 'maintain', 'account', 'cooperate']\n",
      "Topic 40: ['commitment', 'summit', 'significant', 'multilateral', 'point', 'protection', 'achieving', 'suffering', 'questions']\n",
      "Topic 41: ['republic', 'implementation', 'institutions', 'conflicts', 'family', 'asia', 'representation', 'strengthened', 'ideological']\n",
      "Topic 42: ['resolution', 'arab', 'population', 'palestine', 'effectiveness', 'natural', 'final', 'water', 'illegal']\n",
      "Topic 43: ['remains', 'armed', 'give', 'open', 'growing', 'violence', 'minister', 'current', 'racist']\n",
      "Topic 44: ['regime', 'bring', 'solutions', 'island', 'create', 'existence', 'impact', 'vital', 'path']\n",
      "Topic 45: ['effective', 'small', 'history', 'greater', 'soviet', 'act', 'achieve', 'behalf', 'purposes']\n",
      "Topic 46: ['union', 'importance', 'treaty', 'committed', 'attention', 'requires', 'office', 'aspirations', 'needed']\n",
      "Topic 47: ['regional', 'programme', 'fully', 'areas', 'rule', 'period', 'reached', 'beginning', 'effectively']\n",
      "Topic 48: ['problem', 'measures', 'comprehensive', 'steps', 'china', 'genuine', 'levels', 'elimination', 'effect']\n",
      "Topic 49: ['change', 'energy', 'living', 'decade', 'commission', 'cultural', 'side', 'bringing', 'citizens']\n",
      "Current memory usage is 0.927445MB; Peak was 107.065803MB\n"
     ]
    }
   ],
   "source": [
    "from src.main_ETM import main_ETM\n",
    "main_ETM(dataset='un-general-debates', data_path='data/un-general-debates', save_path='data',\n",
    "         emb_path='data/un-general-debates_embeddings.txt', mode='eval',\n",
    "         load_from='data/etm_un-general-debates_K_50_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0',\n",
    "         num_topics=50, train_embeddings=0, epochs=100, visualize_every=1000, num_words=10, tc=True, td=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.iii. Dynamic Embedded Topic Model (DETM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>BRIEF DESCRIPTION OF THE MODEL</font><br>\n",
    "<font color='blue'>MODEL ESTIMATION</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main_DETM import main_DETM\n",
    "main_DETM(dataset='un-general-debates', data_path='data/un-general-debates', save_path='data',\n",
    "          emb_path='data/un-general-debates_embeddings.txt',\n",
    "          num_topics=10, train_embeddings=0, epochs=50, visualize_every=1000, tc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION ON HOW WE WANT TO COMPARE MODELS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The idea is introduced very well in \"Topic modeling in embedding spaces\": let's copy from there!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.i. Quantitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>COMPUTATION OF VARIOUS METRICS AND CONSTRUCTION OF GRAPHS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.ii. Qualitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>INTERPRETATION OF TOPICS AND DOCUMENT REPRESENTATION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>FINAL REMARKS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2019). The dynamic embedded topic model. arXiv preprint arXiv:1907.05545. [Arxiv link](https://arxiv.org/abs/1907.05545)\n",
    "\n",
    "Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2020). Topic modeling in embedding spaces. Transactions of the Association for Computational Linguistics, 8, 439-453. [ACM Anthology](https://aclanthology.org/2020.tacl-1.29/),  [Arxiv link](https://arxiv.org/abs/1907.04907)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmda",
   "language": "python",
   "name": "pmda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
