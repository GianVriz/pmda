{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Metodologies for Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "- Lorenzo Dell'Oro\n",
    "- Giovanni Toto\n",
    "- Gian Luca Vriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>KEY IDEA AND OBJECTIVE OF THE PROJECT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>LIBRARIES</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                                   # 2\n",
    "import string                                # 2\n",
    "from src.preprocessing import preprocessing  # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION TO DATA</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>PRE-PROCESSING OF TEXTS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import the dataset from a file (txt/csv/...); this is an example with [UN General Debates corpus](https://www.kaggle.com/datasets/unitednations/un-general-debates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DOCS = 1000\n",
    "min_df = 10\n",
    "\n",
    "# Data type\n",
    "flag_split_by_paragraph = False  # whether to split documents by paragraph\n",
    "    \n",
    "# Read raw data (https://www.kaggle.com/datasets/unitednations/un-general-debates)\n",
    "print('reading raw data...')\n",
    "with open('./data/raw/un-general-debates.csv', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',', quotechar='\"')\n",
    "    line_count = 0\n",
    "    all_timestamps_ini = []\n",
    "    all_docs_ini = []\n",
    "    for row in csv_reader:\n",
    "        # skip header\n",
    "        if(line_count>0):\n",
    "            all_timestamps_ini.append(row[1])\n",
    "            all_docs_ini.append(row[3].encode(\"ascii\", \"ignore\").decode())\n",
    "        line_count += 1\n",
    "        if line_count==N_DOCS-1:  ###########\n",
    "            break                 ###########\n",
    "\n",
    "if flag_split_by_paragraph:\n",
    "    print('splitting by paragraphs...')\n",
    "    docs = []\n",
    "    timestamps = []\n",
    "    for dd, doc in enumerate(all_docs_ini):\n",
    "        splitted_doc = doc.split('.\\n')\n",
    "        for ii in splitted_doc:\n",
    "            docs.append(ii)\n",
    "            timestamps.append(all_timestamps_ini[dd])\n",
    "else:\n",
    "    docs = all_docs_ini\n",
    "    timestamps = all_timestamps_ini\n",
    "\n",
    "del all_docs_ini\n",
    "del all_timestamps_ini\n",
    "\n",
    "print('  number of documents: {}'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to pre-processing with gensim: [link](https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#from-strings-to-vectors)\n",
    "\n",
    "The real pre-processing starts here: in short, we want to get strings without strange characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "print('removing punctuation...')\n",
    "docs = [[w.lower().replace(\"’\", \" \").replace(\"'\", \" \").translate(str.maketrans('', '', string.punctuation + \"0123456789\")) for w in docs[doc].split()] for doc in range(len(docs))]\n",
    "docs = [[w for w in docs[doc] if len(w)>1] for doc in range(len(docs))]\n",
    "docs = [\" \".join(docs[doc]) for doc in range(len(docs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use `preprocessing` function contained in `src/preprocessing.py` module, which creates files compatible with *ETM* and *DETM*. We will use these files also to perform explorative analysis. Before launching the function, we need to import stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stopwords\n",
    "with open(\"./data/stops.txt\", \"r\") as f:\n",
    "    stopwords = f.read().split('\\n')\n",
    "# Pre-processing\n",
    "preprocessing(data_path=\"data/un-general-debates\", docs=docs, timestamps=timestamps, stopwords=stopwords,\n",
    "              min_df=min_df, max_df=0.7, data_split=[0.85, 0.1, 0.05], seed=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark1:** `docs` and `timestamps` are two lists of strings containing the same number of elements; in particular, `docs` contains the documents of the corpus, `timestamps` their timestamps.\n",
    "\n",
    "**remark2:** The function also divides the corpus into train, test and validation set: below we will consider the training set for exploratory analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory analysis of the processed corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "EXPLORATORY ANALYSIS OF THE **TRAIN** CORPUS:\n",
    "\n",
    "- tabella con info corpus (num documenti, num timestamps, documenti per timestamp, ...)\n",
    "- distribuzione lunghezza documenti (numero parole)\n",
    "- parole più frequenti (word cloud)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import vocabulary of the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.file_io import load_vocab\n",
    "word2id, id2word = load_vocab(\"data/un-general-debates/vocab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider the train corpus only, i.e. we are interested in the following files generated by `preprocessing` function:\n",
    "- `bow_tr_tokens`: index of the different words observed in the documents of the train set;\n",
    "- `bow_tr_counts`: occurrences of the different words observed in the documents of the train set;\n",
    "- `bow_tr_timestamps`: timestamps of the documents of the train set;\n",
    "- `timestamps.txt`: different observed timestamps;\n",
    "- `vocab.txt`: vocabulary of the train set.\n",
    "\n",
    "**remark:** the first 3 files exist also for test and validation set, however they are not relevant to exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "path = os.path.join('data', 'un-general-debates')\n",
    "bow_tr_tokens = loadmat(os.path.join(path, 'bow_tr_tokens'))['tokens'].squeeze()\n",
    "bow_tr_counts = loadmat(os.path.join(path, 'bow_tr_counts'))['counts'].squeeze()\n",
    "bow_tr_timestamps = loadmat(os.path.join(path, 'bow_tr_timestamps'))['timestamps'].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(bow_tr_timestamps))\n",
    "print(len(bow_tr_counts))\n",
    "print(len(bow_tr_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"print(bow_tr_timestamps)\n",
    "for i in range(8):\n",
    "    print(i, \"\\t\", bow_tr_tokens[i].shape, \"\\t\", bow_tr_counts[i].shape)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embeddings and topic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION TO MODEL ESTIMATION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.i. Fitting embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>BRIEF DESCRIPTION OF THE DIFFERENT APPROACHES</font><br>\n",
    "<font color='blue'>EMBEDDING FITTING</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the use the same embedding space for both *ETM* and *DETM*, so we first fit the word embeddings and then we provide them as input. In particular, we fit a simple *skipgram*; this implementation is an adaptation of this [code](https://github.com/adjidieng/ETM/blob/master/skipgram.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bow = [docs[doc].split() for doc in range(len(docs))]  # list of list of strings (BoW representation)\n",
    "\n",
    "# fit embeddings\n",
    "from gensim.models import Word2Vec\n",
    "skipgram = Word2Vec(sentences=docs_bow, min_count=100, sg=1, size=100, iter=5, workers=5, negative=10, window=4)\n",
    "\n",
    "from src.file_io import save_embeddings\n",
    "save_embeddings(emb_model=skipgram, emb_file='data/un-general-debates_embeddings.txt', vocab=list(word2id.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.ii. Embedded Topic Model (ETM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>BRIEF DESCRIPTION OF THE MODEL</font><br>\n",
    "<font color='blue'>MODEL ESTIMATION</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block allows to train *ETM*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.main_ETM import main_ETM\n",
    "main_ETM(dataset='un-general-debates', data_path='data/un-general-debates', save_path='results',\n",
    "         emb_file='data/un-general-debates_embeddings.txt', model_file='ETM_K50_un-general-debates', batch_size=2000,\n",
    "         mode='train', num_topics=50, train_embeddings=0, epochs=100, visualize_every=1000, tc=False, td=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block allows to evaluate *ETM*, i.e.,\n",
    "- compute *topic coherence* on the top 10 words of each topic;\n",
    "- compute *topic diversity* on the top 25 words of each topic,\n",
    "- compute the ranking of the most used topics in the train corpus;\n",
    "- compute the top `num_words` words per topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.iii. Dynamic Embedded Topic Model (DETM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>BRIEF DESCRIPTION OF THE MODEL</font><br>\n",
    "<font color='blue'>MODEL ESTIMATION</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory problems:** DETM rquires too much memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.main_DETM import main_DETM\n",
    "main_DETM(dataset='un-general-debates', data_path='data/un-general-debates', save_path='data',\n",
    "          emb_path='data/un-general-debates_embeddings.txt', mode='train', batch_size=100,\n",
    "          num_topics=50, train_embeddings=0, epochs=50, visualize_every=1000, tc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION ON HOW WE WANT TO COMPARE MODELS</font>\n",
    "**The idea is introduced very well in \"Topic modeling in embedding spaces\": let's copy from there!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block allows to evaluate the model trained in the previous section; in particular, it saves a file, called `<model_name>_parameters.pt` containing:\n",
    "- `tc` contains the topic coherence\n",
    "- `td` contains the topic diversity\n",
    "- `rho` contains the word embeddings (row=embedding)\n",
    "- `model.alphas.weight` contains the topic embeddings (row=embedding)\n",
    "- `beta` contains the topic-word distributions (row=distribution)\n",
    "- `theta` contains the document-topic distribution (row=distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.main_ETM import main_ETM\n",
    "main_ETM(dataset='un-general-debates', data_path='data/un-general-debates', save_path='results',\n",
    "         emb_file='data/un-general-debates_embeddings.txt', model_file='ETM_K50_un-general-debates', mode='eval',\n",
    "         load_from='results/ETM_K50_un-general-debates',\n",
    "         num_topics=50, train_embeddings=0, epochs=100, visualize_every=1000, num_words=10, tc=True, td=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import load as torch_load\n",
    "loaded = torch_load(\"results/ETM_K50_un-general-debates_parameters.pt\")\n",
    "print(loaded.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TC  :\", loaded['tc'])\n",
    "print(\"TD  :\", loaded['td'])\n",
    "print(\"VxE :\", loaded['rho'].shape)\n",
    "print(\"TxE :\", loaded['alpha'].shape)\n",
    "print(\"TxV :\", loaded['beta'].shape)\n",
    "print(\"DxT :\", loaded['theta'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 2, 3], [2, 3, 1]])\n",
    "display(x)\n",
    "print(np.argsort(x, axis=1))\n",
    "print(np.argsort(-1 * x, axis=1)[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.i. Quantitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>COMPUTATION OF VARIOUS METRICS AND CONSTRUCTION OF GRAPHS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.ii. Qualitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>INTERPRETATION OF TOPICS AND DOCUMENT REPRESENTATION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>FINAL REMARKS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2019). The dynamic embedded topic model. arXiv preprint arXiv:1907.05545. [Arxiv link](https://arxiv.org/abs/1907.05545)\n",
    "\n",
    "Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2020). Topic modeling in embedding spaces. Transactions of the Association for Computational Linguistics, 8, 439-453. [ACM Anthology](https://aclanthology.org/2020.tacl-1.29/),  [Arxiv link](https://arxiv.org/abs/1907.04907)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmda",
   "language": "python",
   "name": "pmda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
