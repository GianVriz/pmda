{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Metodologies for Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "- Lorenzo Dell'Oro\n",
    "- Giovanni Toto\n",
    "- Gian Luca Vriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>KEY IDEA AND OBJECTIVE OF THE PROJECT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>LIBRARIES</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                                   # 2\n",
    "import string                                # 2\n",
    "from src.preprocessing import preprocessing  # 2\n",
    "from src.file_io import load_vocab           # 3\n",
    "import os                                    # 3\n",
    "from scipy.io import loadmat                 # 3\n",
    "from gensim.models import Word2Vec           # 4\n",
    "from src.file_io import save_embeddings      # 4\n",
    "from gensim.models import LdaModel           # 4.i\n",
    "from collections import Counter              # 4.ii\n",
    "from gensim.models import ldaseqmodel        # 4.ii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION TO DATA</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>PRE-PROCESSING OF TEXTS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import the dataset from a file (txt/csv/...); this is an example with [UN General Debates corpus](https://www.kaggle.com/datasets/unitednations/un-general-debates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DOCS = 9999999\n",
    "min_df = 100\n",
    "\n",
    "# Data type\n",
    "flag_split_by_paragraph = False  # whether to split documents by paragraph\n",
    "    \n",
    "# Read raw data (https://www.kaggle.com/datasets/unitednations/un-general-debates)\n",
    "print('reading raw data...')\n",
    "with open('./data/raw/un-general-debates.csv', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',', quotechar='\"')\n",
    "    line_count = 0\n",
    "    all_timestamps_ini = []\n",
    "    all_docs_ini = []\n",
    "    for row in csv_reader:\n",
    "        # skip header\n",
    "        if(line_count>0):\n",
    "            all_timestamps_ini.append(row[1])\n",
    "            all_docs_ini.append(row[3].encode(\"ascii\", \"ignore\").decode())\n",
    "        line_count += 1\n",
    "        if line_count==N_DOCS-1:  ###########\n",
    "            break                 ###########\n",
    "\n",
    "if flag_split_by_paragraph:\n",
    "    print('splitting by paragraphs...')\n",
    "    docs = []\n",
    "    timestamps = []\n",
    "    for dd, doc in enumerate(all_docs_ini):\n",
    "        splitted_doc = doc.split('.\\n')\n",
    "        for ii in splitted_doc:\n",
    "            docs.append(ii)\n",
    "            timestamps.append(all_timestamps_ini[dd])\n",
    "else:\n",
    "    docs = all_docs_ini\n",
    "    timestamps = all_timestamps_ini\n",
    "\n",
    "del all_docs_ini\n",
    "del all_timestamps_ini\n",
    "\n",
    "print('  number of documents: {}'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to pre-processing with gensim: [link](https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#from-strings-to-vectors)\n",
    "\n",
    "The real pre-processing starts here: in short, we want to get strings without strange characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "print('removing punctuation...')\n",
    "docs = [[w.replace('\\ufeff', '').lower().replace(\"â€™\", \" \").replace(\"'\", \" \").translate(str.maketrans('', '', string.punctuation + \"0123456789\")) for w in docs[doc].split()] for doc in range(len(docs))]\n",
    "docs = [[w for w in docs[doc] if len(w)>1] for doc in range(len(docs))]\n",
    "docs = [\" \".join(docs[doc]) for doc in range(len(docs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use `preprocessing` function contained in `src/preprocessing.py` module, which creates files compatible with *ETM* and *DETM*. We will use these files also to perform explorative analysis and the estimation of *LDA* and *DTM*. Before launching the function, we need to import stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stopwords\n",
    "with open(\"./data/stops.txt\", \"r\") as f:\n",
    "    stopwords = f.read().split('\\n')\n",
    "# Pre-processing\n",
    "preprocessing(data_path=\"data/un-general-debates\", docs=docs, timestamps=timestamps, stopwords=stopwords,\n",
    "              min_df=min_df, max_df=0.7, data_split=[0.85, 0.1, 0.05], seed=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function also divides the corpus into train, test and validation set: below we will consider the training set for exploratory analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory analysis of the processed corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>EXPLORATORY ANALYSIS OF THE **TRAIN** CORPUS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import vocabulary of the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id, id2word = load_vocab(\"data/un-general-debates/min_df_\" + str(min_df) +\"/vocab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider the train corpus only, i.e. we are interested in the following files generated by `preprocessing` function:\n",
    "- `bow_tr_tokens`: index of the different words observed in the documents of the train set;\n",
    "- `bow_tr_counts`: occurrences of the different words observed in the documents of the train set;\n",
    "- `bow_tr_timestamps`: timestamps of the documents of the train set;\n",
    "- `timestamps.txt`: different observed timestamps;\n",
    "- `vocab.txt`: vocabulary of the train set.\n",
    "\n",
    "**remark:** the first 3 files exist also for test and validation set, however they are not relevant to exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('data', 'un-general-debates', 'min_df_'+str(min_df))\n",
    "bow_tr_tokens = loadmat(os.path.join(path, 'bow_tr_tokens'))['tokens'].squeeze()\n",
    "bow_tr_counts = loadmat(os.path.join(path, 'bow_tr_counts'))['counts'].squeeze()\n",
    "bow_tr_timestamps = loadmat(os.path.join(path, 'bow_tr_timestamps'))['timestamps'].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(bow_tr_timestamps))\n",
    "print(len(bow_tr_counts))\n",
    "print(len(bow_tr_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"print(bow_tr_timestamps)\n",
    "for i in range(8):\n",
    "    print(i, \"\\t\", bow_tr_tokens[i].shape, \"\\t\", bow_tr_counts[i].shape)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estimation of the topic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION TO MODEL ESTIMATION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the use the same embedding space for both *ETM* and *DETM*, so we first fit the word embeddings and then we provide them as input. In particular, we fit a simple *skipgram*; this implementation is an adaptation of this [code](https://github.com/adjidieng/ETM/blob/master/skipgram.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bow = [docs[doc].split() for doc in range(len(docs))]\n",
    "# fit embeddings\n",
    "skipgram = Word2Vec(sentences=docs_bow, min_count=100, sg=1, size=100, iter=5, workers=5, negative=10, window=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(emb_model=skipgram, emb_file='data/un-general-debates_embeddings.txt', vocab=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.i. Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>BRIEF DESCRIPTION OF THE MODEL</font><br>\n",
    "<font color='blue'>MODEL ESTIMATION</font><br>\n",
    "*DO THE SAME FOR ALL TOPIC MODELS*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We report here a simple example: **instead of using `common_texts`, in the project we should use `docs` and `id2word`; the latter is calculated in `preprocessing` function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "id2word = dict(common_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate *LDA* with 5 topics. **\\[check function arguments [here](https://radimrehurek.com/gensim/models/ldamodel.html)\\]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda = LdaModel(common_corpus, id2word=id2word, num_topics=topics, alpha='auto', eta='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.ii. Dynamic Topic Model (DTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain `time_slice` argument of `ldaseqmodel.LdaSeqModel` function from `timestamps` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sorted_times = sorted(set(timestamps))\n",
    "time_slice = Counter(timestamps)\n",
    "time_slice = [time_slice[t] for t in sorted_times]\n",
    "\"\"\"\n",
    "\n",
    "# common_corpus example\n",
    "time_slice = [4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the *DTM* with 5 topics. **\\[check function arguments [here](https://radimrehurek.com/gensim/models/ldaseqmodel.html)\\]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ldaseq = ldaseqmodel.LdaSeqModel(corpus=common_corpus, id2word=id2word, time_slice=time_slice, num_topics=topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.iii. Embedded Topic Model (ETM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "Training an Embedded Topic Model on UN-GENERAL-DEBATES\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "3007  The Vocabulary size is here \n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.0, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (alphas): Linear(in_features=300, out_features=5, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=3007, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=5, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=5, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Visualizing model quality before training... 5\n",
      "\n",
      "\n",
      "I am training for epoch 0\n",
      "The number of the indices I am using for the training is  2\n",
      "Running for  0\n",
      "Running for  1\n",
      "****************************************************************************************************\n",
      "Epoch----->0 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 6361.24 .. NELBO: 6361.26\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 2944.4\n",
      "****************************************************************************************************\n",
      "Computing topic coherence...\n",
      "D:  1698\n",
      "k: 0/5\n",
      "k: 1/5\n",
      "k: 2/5\n",
      "k: 3/5\n",
      "k: 4/5\n",
      "counter:  55\n",
      "num topics:  5\n",
      "Topic Coherence is: 0.029971967793732835\n",
      "Computing topic diversity...\n",
      "Topic diveristy is: 0.984\n",
      "The validation scores 2944.4\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "word: africa .. neighbors: ['africa', 'context', 'period', 'alternative', 'sacrifices', 'military', 'removal', 'critical', 'briefly', 'group']\n",
      "word: republic .. neighbors: ['republic', 'lies', 'demonstrates', 'accords', 'expectation', 'diversity', 'ambitions', 'emerged', 'forward', 'forefront']\n",
      "####################################################################################################\n",
      "I am training for epoch 1\n",
      "The number of the indices I am using for the training is  2\n",
      "Running for  0\n",
      "Running for  1\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 6353.63 .. NELBO: 6354.22\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 2881.7\n",
      "****************************************************************************************************\n",
      "Computing topic coherence...\n",
      "D:  1698\n",
      "k: 0/5\n",
      "k: 1/5\n",
      "k: 2/5\n",
      "k: 3/5\n",
      "k: 4/5\n",
      "counter:  55\n",
      "num topics:  5\n",
      "Topic Coherence is: 0.03771592018504715\n",
      "Computing topic diversity...\n",
      "Topic diveristy is: 0.96\n",
      "The validation scores 2881.7\n",
      "I am training for epoch 2\n",
      "The number of the indices I am using for the training is  2\n",
      "Running for  0\n",
      "Running for  1\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 1.22 .. Rec_loss: 6308.59 .. NELBO: 6309.81\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 2837.8\n",
      "****************************************************************************************************\n",
      "Computing topic coherence...\n",
      "D:  1698\n",
      "k: 0/5\n",
      "k: 1/5\n",
      "k: 2/5\n",
      "k: 3/5\n",
      "k: 4/5\n",
      "counter:  55\n",
      "num topics:  5\n",
      "Topic Coherence is: 0.04283088518812526\n",
      "Computing topic diversity...\n",
      "Topic diveristy is: 0.936\n",
      "The validation scores 2837.8\n",
      "I am training for epoch 3\n",
      "The number of the indices I am using for the training is  2\n",
      "Running for  0\n",
      "Running for  1\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 2.06 .. Rec_loss: 6306.98 .. NELBO: 6309.04\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 2802.2\n",
      "****************************************************************************************************\n",
      "Computing topic coherence...\n",
      "D:  1698\n",
      "k: 0/5\n",
      "k: 1/5\n",
      "k: 2/5\n",
      "k: 3/5\n",
      "k: 4/5\n",
      "counter:  55\n",
      "num topics:  5\n",
      "Topic Coherence is: 0.04012291822636296\n",
      "Computing topic diversity...\n",
      "Topic diveristy is: 0.904\n",
      "The validation scores 2802.2\n",
      "I am training for epoch 4\n",
      "The number of the indices I am using for the training is  2\n",
      "Running for  0\n",
      "Running for  1\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 2.12 .. Rec_loss: 6316.52 .. NELBO: 6318.64\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 2770.7\n",
      "****************************************************************************************************\n",
      "Computing topic coherence...\n",
      "D:  1698\n",
      "k: 0/5\n",
      "k: 1/5\n",
      "k: 2/5\n",
      "k: 3/5\n",
      "k: 4/5\n",
      "counter:  55\n",
      "num topics:  5\n",
      "Topic Coherence is: 0.041722507283888356\n",
      "Computing topic diversity...\n",
      "Topic diveristy is: 0.904\n",
      "The validation scores 2770.7\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 2770.7\n",
      "****************************************************************************************************\n",
      "Computing topic coherence...\n",
      "D:  1698\n",
      "k: 0/5\n",
      "k: 1/5\n",
      "k: 2/5\n",
      "k: 3/5\n",
      "k: 4/5\n",
      "counter:  55\n",
      "num topics:  5\n",
      "Topic Coherence is: 0.041722507283888356\n",
      "Computing topic diversity...\n",
      "Topic diveristy is: 0.904\n",
      "Current memory usage is 0.791468MB; Peak was 24.832293MB\n"
     ]
    }
   ],
   "source": [
    "from src.main_ETM import main_ETM\n",
    "main_ETM(dataset='un-general-debates', data_path='data/un-general-debates', save_path='data',\n",
    "         emb_path='data/un-general-debates_embeddings.txt', min_df=100,\n",
    "         num_topics=5, train_embeddings=0, epochs=5, visualize_every=10, tc=True, td=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.iv. Dynamic Embedded Topic Model (DETM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vocabulary ...\n",
      "Getting training data ...\n",
      "idx: 0/2\n",
      "Getting validation data ...\n",
      "idx: 0/1\n",
      "Getting testing data ...\n",
      "idx: 0/1\n",
      "idx: 0/1\n",
      "idx: 0/1\n",
      "Getting embeddings ...\n",
      "\n",
      "\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "Training a Dynamic Embedded Topic Model on UN-GENERAL-DEBATES\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "\n",
      "DETM architecture: DETM(\n",
      "  (t_drop): Dropout(p=0.0, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=3012, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=5, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=5, bias=True)\n",
      "  (q_eta_map): Linear(in_features=3007, out_features=200, bias=True)\n",
      "  (q_eta): LSTM(200, 200, num_layers=3)\n",
      "  (mu_q_eta): Linear(in_features=205, out_features=5, bias=True)\n",
      "  (logsigma_q_eta): Linear(in_features=205, out_features=5, bias=True)\n",
      ")\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 3246.04 .. KL_eta: 11300.41 .. KL_alpha: 10231865.5 .. Rec_loss: 18589527.0 .. NELBO: 28835939.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL PPL: nan\n",
      "****************************************************************************************************\n",
      "val_ppl:  nan\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 1551.95 .. KL_eta: 2728.01 .. KL_alpha: 10165449.0 .. Rec_loss: 18522470.0 .. NELBO: 28692199.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL PPL: nan\n",
      "****************************************************************************************************\n",
      "val_ppl:  nan\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 1797.68 .. KL_eta: 736.51 .. KL_alpha: 9988418.5 .. Rec_loss: 18535824.0 .. NELBO: 28526777.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL PPL: nan\n",
      "****************************************************************************************************\n",
      "val_ppl:  nan\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 1353.48 .. KL_eta: 167.18 .. KL_alpha: 9901539.0 .. Rec_loss: 18515002.0 .. NELBO: 28418062.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL PPL: nan\n",
      "****************************************************************************************************\n",
      "val_ppl:  nan\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\detm_un-general-debates_K_5_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_L_3_minDF_100_trainEmbeddings_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8c0b861e3187>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m main_DETM(dataset='un-general-debates', data_path='data/un-general-debates', save_path='data',\n\u001b[0;32m      3\u001b[0m           \u001b[0memb_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'data/un-general-debates_embeddings.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m           num_topics=5, train_embeddings=0, epochs=5, visualize_every=5, tc=False)\n\u001b[0m",
      "\u001b[1;32mD:\\pmda\\src\\main_DETM.py\u001b[0m in \u001b[0;36mmain_DETM\u001b[1;34m(dataset, data_path, emb_path, save_path, batch_size, min_df, num_topics, rho_size, emb_size, t_hidden_size, theta_act, train_embeddings, eta_nlayers, eta_hidden_size, delta, lr, lr_factor, epochs, mode, optimizer, seed, enc_drop, eta_dropout, clip, nonmono, wdecay, anneal_lr, bow_norm, num_words, log_interval, visualize_every, eval_batch_size, load_from, tc)\u001b[0m\n\u001b[0;32m    422\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mlr_factor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[0mall_val_ppls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_ppl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\detm_un-general-debates_K_5_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_L_3_minDF_100_trainEmbeddings_0'"
     ]
    }
   ],
   "source": [
    "from src.main_DETM import main_DETM\n",
    "main_DETM(dataset='un-general-debates', data_path='data/un-general-debates', save_path='data',\n",
    "          emb_path='data/un-general-debates_embeddings.txt', min_df=100,\n",
    "          num_topics=5, train_embeddings=0, epochs=5, visualize_every=5, tc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION ON HOW WE WANT TO COMPARE MODELS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The idea is introduced very well in \"Topic modeling in embedding spaces\": let's copy from there!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.i. Quantitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>COMPUTATION OF VARIOUS METRICS AND CONSTRUCTION OF GRAPHS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.ii. Qualitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>INTERPRETATION OF TOPICS AND DOCUMENT REPRESENTATION (LDA vs ETM, DTM vs DETM)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>FINAL REMARKS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022. [ACM Digital Library](https://dl.acm.org/doi/10.5555/944919.944937)\n",
    "\n",
    "Blei, D. M., & Lafferty, J. D. (2006, June). Dynamic topic models. In Proceedings of the 23rd international conference on Machine learning (pp. 113-120). [ACM Digital Library](https://doi.org/10.1145/1143844.1143859)\n",
    "\n",
    "Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2019). The dynamic embedded topic model. arXiv preprint arXiv:1907.05545. [Arxiv link](https://arxiv.org/abs/1907.05545)\n",
    "\n",
    "Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2020). Topic modeling in embedding spaces. Transactions of the Association for Computational Linguistics, 8, 439-453. [ACM Anthology](https://aclanthology.org/2020.tacl-1.29/),  [Arxiv link](https://arxiv.org/abs/1907.04907)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmda",
   "language": "python",
   "name": "pmda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
