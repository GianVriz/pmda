{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Metodologies for Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorenzo Dell'Oro<br>Giovanni Toto<br>Gian Luca Vriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>KEY IDEA AND OBJECTIVE OF THE PROJECT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>LIBRARIES</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import string\n",
    "import os\n",
    "# ------------------------------------------ # 1\n",
    "from src.preprocessing import preprocessing  # 2\n",
    "from gensim.models import LdaModel           # 4.i\n",
    "from collections import Counter              # 4.ii\n",
    "from gensim.models import ldaseqmodel        # 4.ii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION TO DATA</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>PRE-PROCESSING OF TEXTS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import the dataset from a file (txt/csv/...); this is an example with [UN General Debates corpus](https://www.kaggle.com/datasets/unitednations/un-general-debates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stopwords\n",
    "with open(\"./data/stops.txt\", \"r\") as f:\n",
    "    stopwords = f.read().split('\\n')\n",
    "\n",
    "# Data type\n",
    "flag_split_by_paragraph = False  # whether to split documents by paragraph\n",
    "\n",
    "# Read stopwords\n",
    "with open(\"./data/stops.txt\", \"r\") as f:\n",
    "    stops = f.read().split('\\n')\n",
    "    \n",
    "# Read raw data (https://www.kaggle.com/datasets/unitednations/un-general-debates)\n",
    "print('reading raw data...')\n",
    "with open('./data/raw/un-general-debates.csv', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',', quotechar='\"')\n",
    "    line_count = 0\n",
    "    all_timestamps_ini = []\n",
    "    all_docs_ini = []\n",
    "    for row in csv_reader:\n",
    "        # skip header\n",
    "        if(line_count>0):\n",
    "            all_timestamps_ini.append(row[1])\n",
    "            all_docs_ini.append(row[3])\n",
    "        line_count += 1\n",
    "        if line_count==11:  ###########\n",
    "            break           ###########\n",
    "\n",
    "if flag_split_by_paragraph:\n",
    "    print('splitting by paragraphs...')\n",
    "    docs = []\n",
    "    timestamps = []\n",
    "    for dd, doc in enumerate(all_docs_ini):\n",
    "        splitted_doc = doc.split('.\\n')\n",
    "        for ii in splitted_doc:\n",
    "            docs.append(ii)\n",
    "            timestamps.append(all_timestamps_ini[dd])\n",
    "else:\n",
    "    docs = all_docs_ini\n",
    "    timestamps = all_timestamps_ini\n",
    "\n",
    "del all_docs_ini\n",
    "del all_timestamps_ini\n",
    "\n",
    "timestamps[0] = '1900'\n",
    "timestamps[1] = '1900'\n",
    "\n",
    "print('  number of documents: D={}'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to pre-processing with gensim: [link](https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#from-strings-to-vectors)\n",
    "\n",
    "The real pre-processing starts here: in short, we want to get strings without strange characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "print('removing punctuation...')\n",
    "docs = [[w.replace('\\ufeff', '').lower().replace(\"â€™\", \" \").replace(\"'\", \" \").translate(str.maketrans('', '', string.punctuation + \"0123456789\")) for w in docs[doc].split()] for doc in range(len(docs))]\n",
    "docs = [[w for w in docs[doc] if len(w)>1] for doc in range(len(docs))]\n",
    "docs = [\" \".join(docs[doc]) for doc in range(len(docs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use `preprocessing` function contained in `src/preprocessing.py` module, which creates files compatible with *ETM* and *DETM*. We will use these files also to perform explorative analysis and the estimation of *LDA* and *DTM*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(data_path=\"data/un-general-debates\", docs=docs, timestamps=timestamps, stopwords=stopwords,\n",
    "              min_df=1, max_df=0.7, data_split=[0.85, 0.1, 0.05], seed=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function also divides the corpus into train, test and validation set: below we will consider the training set for exploratory analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory analysis of the processed corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>EXPLORATORY ANALYSIS OF THE CORPUS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estimation of the topic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION TO MODEL ESTIMATION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.i. Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>BRIEF DESCRIPTION OF THE MODEL</font><br>\n",
    "<font color='blue'>MODEL ESTIMATION</font><br>\n",
    "*DO THE SAME FOR ALL TOPIC MODELS*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We report here a simple example: **instead of using `common_texts`, in the project we should use `docs` and `id2word`; the latter is calculated in `preprocessing` function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel  # gia' segnato sopra\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "id2word = dict(common_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate *LDA* with 5 topics. **\\[check function arguments [here](https://radimrehurek.com/gensim/models/ldamodel.html)\\]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(common_corpus, id2word=id2word, num_topics=topics, alpha='auto', eta='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.ii. Dynamic Topic Model (DTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain `time_slice` argument of `ldaseqmodel.LdaSeqModel` function from `timestamps` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sorted_times = sorted(set(timestamps))\n",
    "time_slice = Counter(timestamps)\n",
    "time_slice = [time_slice[t] for t in sorted_times]\n",
    "\"\"\"\n",
    "\n",
    "# common_corpus example\n",
    "time_slice = [4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the *DTM* with 5 topics. **\\[check function arguments [here](https://radimrehurek.com/gensim/models/ldaseqmodel.html)\\]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaseq = ldaseqmodel.LdaSeqModel(corpus=common_corpus, id2word=id2word, time_slice=time_slice, num_topics=topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.iii. Embedded Topic Model (ETM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.iv. Dynamic Embedded Topic Model (DETM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>INTRODUCTION ON HOW WE WANT TO COMPARE MODELS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The idea is introduced very well in \"Topic modeling in embedding spaces\": let's copy from there!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.i. Quantitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>COMPUTATION OF VARIOUS METRICS AND CONSTRUCTION OF GRAPHS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.ii. Qualitative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>INTERPRETATION OF TOPICS AND DOCUMENT REPRESENTATION (LDA vs ETM, DTM vs DETM)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>FINAL REMARKS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022. [ACM Digital Library](https://dl.acm.org/doi/10.5555/944919.944937)\n",
    "\n",
    "Blei, D. M., & Lafferty, J. D. (2006, June). Dynamic topic models. In Proceedings of the 23rd international conference on Machine learning (pp. 113-120). [ACM Digital Library](https://doi.org/10.1145/1143844.1143859)\n",
    "\n",
    "Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2019). The dynamic embedded topic model. arXiv preprint arXiv:1907.05545. [Arxiv link](https://arxiv.org/abs/1907.05545)\n",
    "\n",
    "Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2020). Topic modeling in embedding spaces. Transactions of the Association for Computational Linguistics, 8, 439-453. [ACM Anthology](https://aclanthology.org/2020.tacl-1.29/),  [Arxiv link](https://arxiv.org/abs/1907.04907)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmda",
   "language": "python",
   "name": "pmda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
