{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import string\n",
    "import os\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(28)\n",
    "random.seed(28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading raw data...\n",
      "  number of documents: D=10\n",
      "removing punctuation...\n",
      "writing to text file...\n"
     ]
    }
   ],
   "source": [
    "# Data type\n",
    "flag_split_by_paragraph = False  # whether to split documents by paragraph\n",
    "\n",
    "# Read stopwords\n",
    "with open(\"./data/stops.txt\", \"r\") as f:\n",
    "    stops = f.read().split('\\n')\n",
    "    \n",
    "# Read raw data (https://www.kaggle.com/datasets/unitednations/un-general-debates)\n",
    "print('reading raw data...')\n",
    "with open('./data/raw/un-general-debates.csv', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',', quotechar='\"')\n",
    "    line_count = 0\n",
    "    all_timestamps_ini = []\n",
    "    all_docs_ini = []\n",
    "    for row in csv_reader:\n",
    "        # skip header\n",
    "        if(line_count>0):\n",
    "            all_timestamps_ini.append(row[1])\n",
    "            all_docs_ini.append(row[3])\n",
    "        line_count += 1\n",
    "        if line_count==11:  ###########\n",
    "            break           ###########\n",
    "\n",
    "if flag_split_by_paragraph:\n",
    "    print('splitting by paragraphs...')\n",
    "    docs = []\n",
    "    timestamps = []\n",
    "    for dd, doc in enumerate(all_docs_ini):\n",
    "        splitted_doc = doc.split('.\\n')\n",
    "        for ii in splitted_doc:\n",
    "            docs.append(ii)\n",
    "            timestamps.append(all_timestamps_ini[dd])\n",
    "else:\n",
    "    docs = all_docs_ini\n",
    "    timestamps = all_timestamps_ini\n",
    "\n",
    "del all_docs_ini\n",
    "del all_timestamps_ini\n",
    "\n",
    "timestamps[0] = '1900'\n",
    "timestamps[1] = '1900'\n",
    "\n",
    "print('  number of documents: D={}'.format(len(docs)))\n",
    "\n",
    "# Remove punctuation\n",
    "print('removing punctuation...')\n",
    "docs = [[w.replace('\\ufeff', '').lower().replace(\"’\", \" \").replace(\"'\", \" \").translate(str.maketrans('', '', string.punctuation + \"0123456789\")) for w in docs[doc].split()] for doc in range(len(docs))]\n",
    "docs = [[w for w in docs[doc] if len(w)>1] for doc in range(len(docs))]\n",
    "docs = [\" \".join(docs[doc]) for doc in range(len(docs))]\n",
    "\n",
    "# Write as raw text\n",
    "out_filename = './data/all_docs_splitParagraphs' + str(flag_split_by_paragraph) + '.txt'\n",
    "print('writing to text file...')\n",
    "with open(out_filename, 'w', encoding='utf-8-sig') as f:\n",
    "    for line in docs:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora ho due liste di lunghezza $D$, i.e., il numero di documenti:\n",
    "- `timestamps` contiene l'intervallo temporale di appartenenza\n",
    "- `docs` contiene i documenti (stringhe)\n",
    "\n",
    "Inoltre, i testi dei documenti sono stati salvati nel file `data/all_docs_splitParagraphsFalse.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *True* pre-processing starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting document frequency of words...\n",
      "building the vocabulary...\n",
      "  initial vocabulary size: V=4348\n",
      "  vocabulary size after removing stopwords from list: V=4104\n"
     ]
    }
   ],
   "source": [
    "# Maximum / minimum document frequency\n",
    "max_df = 0.7  # una parola deve comparire in al massimo il max_df% dei documenti\n",
    "min_df = 1    # una parola deve comparire in almeno min_df documenti\n",
    "\n",
    "# Create count vectorizer\n",
    "print('counting document frequency of words...')\n",
    "cvectorizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=None)\n",
    "cvz = cvectorizer.fit_transform(docs).sign()  # (D, V) sparse matrix\n",
    "\n",
    "# Get vocabulary\n",
    "print('building the vocabulary...')\n",
    "sum_counts = cvz.sum(axis=0)  # (1, V) numpy matrix\n",
    "v_size = sum_counts.shape[1]  # V = v_size\n",
    "sum_counts_np = np.zeros(v_size, dtype=int)\n",
    "for v in range(v_size):\n",
    "    sum_counts_np[v] = sum_counts[0,v]\n",
    "word2id = dict([(w, cvectorizer.vocabulary_.get(w)) for w in cvectorizer.vocabulary_])  # dict with V elements\n",
    "id2word = dict([(cvectorizer.vocabulary_.get(w), w) for w in cvectorizer.vocabulary_])  # dict with V elements\n",
    "del cvectorizer\n",
    "print('  initial vocabulary size: V={}'.format(v_size))\n",
    "\n",
    "# Sort elements in vocabulary\n",
    "idx_sort = np.argsort(sum_counts_np)\n",
    "vocab_aux = [id2word[idx_sort[cc]] for cc in range(v_size)]\n",
    "\n",
    "# Filter out stopwords (if any)\n",
    "vocab_aux = [w for w in vocab_aux if w not in stops]\n",
    "print('  vocabulary size after removing stopwords from list: V={}'.format(len(vocab_aux)))\n",
    "\n",
    "# Create dictionary and inverse dictionary\n",
    "vocab = vocab_aux\n",
    "del vocab_aux\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ho definito il vocabolario di $V$ parole (`vocab`) e ora ho due dizionari contenenti le mappe word-id (`word2id`) e id-word (`id2word`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping of timestamps\n",
    "all_times = sorted(set(timestamps))\n",
    "time2id = dict([(t, i) for i, t in enumerate(all_times)])  # dict with different observed times\n",
    "id2time = dict([(i, t) for i, t in enumerate(all_times)])  # dict with different observed times\n",
    "time_list = [id2time[i] for i in range(len(all_times))]    # list containing the different observed times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come per il vocabolario, ho delle mappe anche per il tempo, ovvero time-id (`time2id`) e id-time (`id2time`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/test/valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing documents and splitting into train/test/valid...\n",
      "  vocabulary after removing words not in train: 3721\n",
      "  number of documents (train): 8 [this should be equal to 8 and 8]\n",
      "  number of documents (test): 1 [this should be equal to 1 and 1]\n",
      "  number of documents (valid): 1 [this should be equal to 1 and 1]\n"
     ]
    }
   ],
   "source": [
    "# Split in train/test/valid\n",
    "print('tokenizing documents and splitting into train/test/valid...')\n",
    "num_docs = cvz.shape[0]\n",
    "trSize = int(np.floor(0.85*num_docs))\n",
    "tsSize = int(np.floor(0.10*num_docs))\n",
    "vaSize = int(num_docs - trSize - tsSize)\n",
    "del cvz\n",
    "idx_permute = np.random.permutation(num_docs).astype(int)\n",
    "\n",
    "# Remove words not in train_data\n",
    "vocab = list(set([w for idx_d in range(trSize) for w in docs[idx_permute[idx_d]].split() if w in word2id]))\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])\n",
    "print('  vocabulary after removing words not in train: {}'.format(len(vocab)))\n",
    "\n",
    "# Bag-of-Words (BoW) representation dei documenti, i.e.,  docs_tr/docs_ts/docs_va sono liste di liste di id\n",
    "docs_tr = [[word2id[w] for w in docs[idx_permute[idx_d]].split() if w in word2id] for idx_d in range(trSize)]\n",
    "timestamps_tr = [time2id[timestamps[idx_permute[idx_d]]] for idx_d in range(trSize)]\n",
    "docs_ts = [[word2id[w] for w in docs[idx_permute[idx_d+trSize]].split() if w in word2id] for idx_d in range(tsSize)]\n",
    "timestamps_ts = [time2id[timestamps[idx_permute[idx_d+trSize]]] for idx_d in range(tsSize)]\n",
    "docs_va = [[word2id[w] for w in docs[idx_permute[idx_d+trSize+tsSize]].split() if w in word2id] for idx_d in range(vaSize)]\n",
    "timestamps_va = [time2id[timestamps[idx_permute[idx_d+trSize+tsSize]]] for idx_d in range(vaSize)]\n",
    "#del docs\n",
    "\n",
    "print('  number of documents (train): {} [this should be equal to {} and {}]'.format(len(docs_tr), trSize, len(timestamps_tr)))\n",
    "print('  number of documents (test): {} [this should be equal to {} and {}]'.format(len(docs_ts), tsSize, len(timestamps_ts)))\n",
    "print('  number of documents (valid): {} [this should be equal to {} and {}]'.format(len(docs_va), vaSize, len(timestamps_va)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ho diviso il dataset in train/test/valid e ho modificato il vocabolario in modo da considerare solo le parole del train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing empty documents...\n",
      "splitting test documents in 2 halves...\n"
     ]
    }
   ],
   "source": [
    "# Remove empty documents\n",
    "print('removing empty documents...')\n",
    "\n",
    "def remove_empty(in_docs, in_timestamps):\n",
    "    out_docs = []\n",
    "    out_timestamps = []\n",
    "    for ii, doc in enumerate(in_docs):\n",
    "        if(doc!=[]):\n",
    "            out_docs.append(doc)\n",
    "            out_timestamps.append(in_timestamps[ii])\n",
    "    return out_docs, out_timestamps\n",
    "\n",
    "def remove_by_threshold(in_docs, in_timestamps, thr):\n",
    "    \"\"\"\n",
    "    rimuovo i documenti con numero di parole\n",
    "    \"\"\"\n",
    "    out_docs = []\n",
    "    out_timestamps = []\n",
    "    for ii, doc in enumerate(in_docs):\n",
    "        if(len(doc)>thr):\n",
    "            out_docs.append(doc)\n",
    "            out_timestamps.append(in_timestamps[ii])\n",
    "    return out_docs, out_timestamps\n",
    "\n",
    "docs_tr, timestamps_tr = remove_empty(docs_tr, timestamps_tr)\n",
    "docs_ts, timestamps_ts = remove_empty(docs_ts, timestamps_ts)\n",
    "docs_va, timestamps_va = remove_empty(docs_va, timestamps_va)\n",
    "\n",
    "# Remove test documents with length=1 (or less)\n",
    "docs_ts, timestamps_ts = remove_by_threshold(docs_ts, timestamps_ts, 1)\n",
    "\n",
    "# Split documents in test set in 2 halves\n",
    "print('splitting test documents in 2 halves...')\n",
    "docs_ts_h1 = [[w for i,w in enumerate(doc) if i<=len(doc)/2.0-1] for doc in docs_ts]\n",
    "docs_ts_h2 = [[w for i,w in enumerate(doc) if i>len(doc)/2.0-1] for doc in docs_ts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visto che ho ridotto il vocabolario, ho ridotto anche la dimensione dei documenti: ho quindi rimosso tutti quelli vuoti e quelli contenenti un numero parole minore di una soglia (`thr=1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating lists of words...\n",
      "  len(words_tr):\t 9760\n",
      "  len(words_ts):\t 451\n",
      "  len(words_ts_h1):\t 225\n",
      "  len(words_ts_h2):\t 226\n",
      "  len(words_va):\t 608\n",
      "getting doc indices...\n",
      "  len(np.unique(doc_indices_tr)): 8 [this should be 8]\n",
      "  len(np.unique(doc_indices_ts)): 1 [this should be 1]\n",
      "  len(np.unique(doc_indices_ts_h1)): 1 [this should be 1]\n",
      "  len(np.unique(doc_indices_ts_h2)): 1 [this should be 1]\n",
      "  len(np.unique(doc_indices_va)): 1 [this should be 1]\n"
     ]
    }
   ],
   "source": [
    "# Getting lists of words and doc_indices\n",
    "print('creating lists of words...')\n",
    "\n",
    "def create_list_words(in_docs):\n",
    "    return [x for y in in_docs for x in y]\n",
    "\n",
    "words_tr = create_list_words(docs_tr)\n",
    "words_ts = create_list_words(docs_ts)\n",
    "words_ts_h1 = create_list_words(docs_ts_h1)\n",
    "words_ts_h2 = create_list_words(docs_ts_h2)\n",
    "words_va = create_list_words(docs_va)\n",
    "\n",
    "print('  len(words_tr):\\t', len(words_tr))\n",
    "print('  len(words_ts):\\t', len(words_ts))\n",
    "print('  len(words_ts_h1):\\t', len(words_ts_h1))\n",
    "print('  len(words_ts_h2):\\t', len(words_ts_h2))\n",
    "print('  len(words_va):\\t', len(words_va))\n",
    "\n",
    "# Get doc indices\n",
    "print('getting doc indices...')\n",
    "\n",
    "def create_doc_indices(in_docs):\n",
    "    aux = [[j for i in range(len(doc))] for j, doc in enumerate(in_docs)]\n",
    "    return [int(x) for y in aux for x in y]\n",
    "\n",
    "doc_indices_tr = create_doc_indices(docs_tr)\n",
    "doc_indices_ts = create_doc_indices(docs_ts)\n",
    "doc_indices_ts_h1 = create_doc_indices(docs_ts_h1)\n",
    "doc_indices_ts_h2 = create_doc_indices(docs_ts_h2)\n",
    "doc_indices_va = create_doc_indices(docs_va)\n",
    "\n",
    "print('  len(np.unique(doc_indices_tr)): {} [this should be {}]'.format(len(np.unique(doc_indices_tr)), len(docs_tr)))\n",
    "print('  len(np.unique(doc_indices_ts)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts)), len(docs_ts)))\n",
    "print('  len(np.unique(doc_indices_ts_h1)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h1)), len(docs_ts_h1)))\n",
    "print('  len(np.unique(doc_indices_ts_h2)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h2)), len(docs_ts_h2)))\n",
    "print('  len(np.unique(doc_indices_va)): {} [this should be {}]'.format(len(np.unique(doc_indices_va)), len(docs_va)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora ho vettori contenenti gli id delle parole in ordine di *train/test/valid set* (`words_tr/words_ts/...`) e vettore della stessa lunghezza che specificano il documento di appartenenza di ogni parola. **(propabilmente il modello lavora su queste variabili per motici di efficienza)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents in each set\n",
    "n_docs_tr = len(docs_tr)\n",
    "n_docs_ts = len(docs_ts)\n",
    "n_docs_ts_h1 = len(docs_ts_h1)\n",
    "n_docs_ts_h2 = len(docs_ts_h2)\n",
    "n_docs_va = len(docs_va)\n",
    "\n",
    "# Remove unused variables\n",
    "del docs_tr\n",
    "del docs_ts\n",
    "del docs_ts_h1\n",
    "del docs_ts_h2\n",
    "del docs_va"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ho salvato il numero di documenti $D$ contenuto in ogni set (`n_docs_tr/n_docs_ts/...`) e poi eliminato variabili inutili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = './data/split_paragraph_' + str(flag_split_by_paragraph) + '/min_df_' + str(min_df) + '/'\n",
    "if not os.path.exists(path_save):\n",
    "    os.makedirs(path_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ho creato la cartella in cui salvare tutti li input: ora posso procedere al salvataggio dei vari file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating bow representation...\n"
     ]
    }
   ],
   "source": [
    "# Create bow representation\n",
    "print('creating bow representation...')\n",
    "\n",
    "def create_bow(doc_indices, words, n_docs, vocab_size):\n",
    "    return sparse.coo_matrix(([1]*len(doc_indices),(doc_indices, words)), shape=(n_docs, vocab_size)).tocsr()\n",
    "\n",
    "bow_tr = create_bow(doc_indices_tr, words_tr, n_docs_tr, len(vocab))\n",
    "bow_ts = create_bow(doc_indices_ts, words_ts, n_docs_ts, len(vocab))\n",
    "bow_ts_h1 = create_bow(doc_indices_ts_h1, words_ts_h1, n_docs_ts_h1, len(vocab))\n",
    "bow_ts_h2 = create_bow(doc_indices_ts_h2, words_ts_h2, n_docs_ts_h2, len(vocab))\n",
    "bow_va = create_bow(doc_indices_va, words_va, n_docs_va, len(vocab))\n",
    "\n",
    "del words_tr\n",
    "del words_ts\n",
    "del words_ts_h1\n",
    "del words_ts_h2\n",
    "del words_va\n",
    "del doc_indices_tr\n",
    "del doc_indices_ts\n",
    "del doc_indices_ts_h1\n",
    "del doc_indices_ts_h2\n",
    "del doc_indices_va"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La seguente parte in cui salvo file per LDA C++ code serve solo per DETM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving LDA files for C++ code...\n"
     ]
    }
   ],
   "source": [
    "# Write files for LDA C++ code\n",
    "def write_lda_file(filename, timestamps_in, time_list_in, bow_in):\n",
    "    idxSort = np.argsort(timestamps_in)\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        for row in idxSort:\n",
    "            x = bow_in.getrow(row)\n",
    "            n_elems = x.count_nonzero()\n",
    "            f.write(str(n_elems))\n",
    "            if(n_elems != len(x.indices) or n_elems != len(x.data)):\n",
    "                print(\"[ERR] THIS SHOULD NOT HAPPEN\")\n",
    "            for ii, dd in zip(x.indices, x.data):\n",
    "                f.write(' ' + str(ii) + ':' + str(dd))\n",
    "            f.write('\\n')\n",
    "            \n",
    "    with open(filename.replace(\"-mult\", \"-seq\"), \"w\") as f:\n",
    "        f.write(str(len(time_list_in)) + '\\n')\n",
    "        for idx_t, _ in enumerate(time_list_in):\n",
    "            n_elem = len([t for t in timestamps_in if t==idx_t])\n",
    "            f.write(str(n_elem) + '\\n')\n",
    "\n",
    "print('saving LDA files for C++ code...')\n",
    "write_lda_file(path_save + 'dtm_tr-mult.dat', timestamps_tr, time_list, bow_tr)\n",
    "write_lda_file(path_save + 'dtm_ts-mult.dat', timestamps_ts, time_list, bow_ts)\n",
    "write_lda_file(path_save + 'dtm_ts_h1-mult.dat', timestamps_ts, time_list, bow_ts_h1)\n",
    "write_lda_file(path_save + 'dtm_ts_h2-mult.dat', timestamps_ts, time_list, bow_ts_h2)\n",
    "write_lda_file(path_save + 'dtm_va-mult.dat', timestamps_va, time_list, bow_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting bow intro token/value pairs and saving to disk...\n",
      "Data ready !!\n",
      "*************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Giovanni\\anaconda3\\envs\\pmda\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "# Also write the vocabulary and timestamps\n",
    "with open(path_save + 'vocab.txt', \"w\") as f:\n",
    "    for v in vocab:\n",
    "        f.write(v + '\\n')\n",
    "\n",
    "with open(path_save + 'timestamps.txt', \"w\") as f:\n",
    "    for t in time_list:\n",
    "        f.write(t + '\\n')\n",
    "\n",
    "with open(path_save + 'vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "del vocab\n",
    "\n",
    "with open(path_save + 'timestamps.pkl', 'wb') as f:\n",
    "    pickle.dump(time_list, f)\n",
    "\n",
    "# Save timestamps alone\n",
    "savemat(path_save + 'bow_tr_timestamps', {'timestamps': timestamps_tr}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_timestamps', {'timestamps': timestamps_ts}, do_compression=True)\n",
    "savemat(path_save + 'bow_va_timestamps', {'timestamps': timestamps_va}, do_compression=True)\n",
    "\n",
    "# Split bow intro token/value pairs\n",
    "print('splitting bow intro token/value pairs and saving to disk...')\n",
    "\n",
    "def split_bow(bow_in, n_docs):\n",
    "    indices = [[w for w in bow_in[doc,:].indices] for doc in range(n_docs)]\n",
    "    counts = [[c for c in bow_in[doc,:].data] for doc in range(n_docs)]\n",
    "    return indices, counts\n",
    "\n",
    "bow_tr_tokens, bow_tr_counts = split_bow(bow_tr, n_docs_tr)\n",
    "savemat(path_save + 'bow_tr_tokens', {'tokens': bow_tr_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_tr_counts', {'counts': bow_tr_counts}, do_compression=True)\n",
    "del bow_tr\n",
    "del bow_tr_tokens\n",
    "del bow_tr_counts\n",
    "\n",
    "bow_ts_tokens, bow_ts_counts = split_bow(bow_ts, n_docs_ts)\n",
    "savemat(path_save + 'bow_ts_tokens', {'tokens': bow_ts_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_counts', {'counts': bow_ts_counts}, do_compression=True)\n",
    "del bow_ts\n",
    "del bow_ts_tokens\n",
    "del bow_ts_counts\n",
    "\n",
    "bow_ts_h1_tokens, bow_ts_h1_counts = split_bow(bow_ts_h1, n_docs_ts_h1)\n",
    "savemat(path_save + 'bow_ts_h1_tokens', {'tokens': bow_ts_h1_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_h1_counts', {'counts': bow_ts_h1_counts}, do_compression=True)\n",
    "del bow_ts_h1\n",
    "del bow_ts_h1_tokens\n",
    "del bow_ts_h1_counts\n",
    "\n",
    "bow_ts_h2_tokens, bow_ts_h2_counts = split_bow(bow_ts_h2, n_docs_ts_h2)\n",
    "savemat(path_save + 'bow_ts_h2_tokens', {'tokens': bow_ts_h2_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_h2_counts', {'counts': bow_ts_h2_counts}, do_compression=True)\n",
    "del bow_ts_h2\n",
    "del bow_ts_h2_tokens\n",
    "del bow_ts_h2_counts\n",
    "\n",
    "bow_va_tokens, bow_va_counts = split_bow(bow_va, n_docs_va)\n",
    "savemat(path_save + 'bow_va_tokens', {'tokens': bow_va_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_va_counts', {'counts': bow_va_counts}, do_compression=True)\n",
    "del bow_va\n",
    "del bow_va_tokens\n",
    "del bow_va_counts\n",
    "\n",
    "print('Data ready !!')\n",
    "print('*************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmda",
   "language": "python",
   "name": "pmda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
